{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time variable to know the time taken to upload the files\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "# Data Import files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables: numFiles is the total number of files per user data\n",
    "# endRow is the limiting of datapoints in the data available\n",
    "numFiles = 30\n",
    "endRow = 70000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depression data\n",
    "dep_data=[]\n",
    "\n",
    "for fileNum in range(1,numFiles+1):\n",
    "    fileNum = f\"{fileNum:02d}\"\n",
    "    location = 'modified_data/depression-mat/leftclosed'+ fileNum +'.txt'\n",
    "    df = np.loadtxt(location)\n",
    "    df=  df[0:endRow]\n",
    "    dep_data.append(df)\n",
    "    \n",
    "dep_data = stats.zscore(dep_data)\n",
    "dep_data = np.asarray(dep_data)\n",
    "dep_data = np.reshape(dep_data,[700, 3000])\n",
    "\n",
    "# Normal data\n",
    "nor_data=[]\n",
    "\n",
    "for fileNum in range(1,numFiles+1):\n",
    "    fileNum = f\"{fileNum:02d}\"\n",
    "    location = 'modified_data/depression-mat/leftclosed'+ fileNum +'.txt'\n",
    "    df = np.loadtxt(location)\n",
    "    df=  df[0:endRow]\n",
    "    nor_data.append(df)\n",
    "    \n",
    "nor_data = stats.zscore(nor_data)\n",
    "nor_data = np.asarray(nor_data)\n",
    "nor_data = np.reshape(nor_data,[700, 3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 6000)\n",
      "(6000, 700)\n"
     ]
    }
   ],
   "source": [
    "# Assembling the data\n",
    "data = np.concatenate((dep_data,nor_data),axis=1)\n",
    "print(np.shape(data)) #Prints the shape of data\n",
    "data = np.transpose(data)\n",
    "print(np.shape(data)) #Prints the shape of transposed data\n",
    "\n",
    "# Building labels\n",
    "label = np.concatenate((np.ones(3000),np.zeros(3000)), axis=0)\n",
    "\n",
    "#shuffling the set\n",
    "data,label =shuffle(data, label, random_state=0) \n",
    "\n",
    "#reshaping the data\n",
    "data= data.reshape(data.shape[0], data.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for data import from dataset.py(in sec):13.575112342834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for data/modules import(in sec):17.392961740493774\n"
     ]
    }
   ],
   "source": [
    "#Import training data and labels from the other python file. In this, the file has already been uploaded\n",
    "#from dataset import data, label\n",
    "\n",
    "# Count elapsed time and print it\n",
    "elapsed = time.time() - t\n",
    "print(\"Time taken for data import from dataset.py(in sec):\"+str(elapsed))\n",
    "\n",
    "#importing modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras import optimizers, metrics\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Count elapsed time for import and print it\n",
    "elapsed = time.time() - t\n",
    "print(\"Time taken for data/modules import(in sec):\"+str(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data shape for input_shape argument.\n",
    "\n",
    "data_shape=data.shape[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/blackwater/anaconda3/envs/ml/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/blackwater/anaconda3/envs/ml/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/blackwater/anaconda3/envs/ml/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/blackwater/anaconda3/envs/ml/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CNN network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters= 32, kernel_size = 5, input_shape=(data_shape,1)))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model.add(Activation(\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/blackwater/anaconda3/envs/ml/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Configures the model for training\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Splitting: Train, test\n",
    "\n",
    "X_Train, X_test, y_Train, y_test = train_test_split(data, label, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train to train, CV\n",
    "\n",
    "X_train, X_CV, y_train, y_CV = train_test_split(X_Train, y_Train, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/blackwater/anaconda3/envs/ml/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/blackwater/anaconda3/envs/ml/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/1\n",
      "4860/4860 [==============================] - 22s 4ms/step - loss: 0.2582 - acc: 0.5023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42fbfb3090>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training : Trains the model for a given number of epochs (iterations on a dataset)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 [==============================] - 2s 3ms/step\n",
      "Evaluation score is  [0.2511443940025789, 0.5018518571224477]\n"
     ]
    }
   ],
   "source": [
    "#evaluate: Returns the loss value & metrics values(accuracy) for the model in test mode.\n",
    "\n",
    "score = model.evaluate(X_CV, y_CV, batch_size=10)\n",
    "print(\"Evaluation score is \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output predictions\n",
      "600/600 [==============================] - 2s 3ms/step\n",
      "Evaluation score is  [0.25271775672833124, 0.48000000876684984]\n"
     ]
    }
   ],
   "source": [
    "#Generates output predictions for the input samples\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=None, verbose=0)\n",
    "print(\"Output predictions\")\n",
    "score = model.evaluate(X_test, y_test, batch_size=10)\n",
    "print(\"Evaluation score is \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions, y_test\n",
    "\n",
    "v = np.empty([600, 2]) \n",
    "v[:,0] = predictions[:,0]\n",
    "v[:,1]= y_test[:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the value to a text file\n",
    "\n",
    "np.savetxt(\"a.txt\",v,fmt='%4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4281258990>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot of y_test vs predictions\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## The line / model\n",
    "plt.scatter(y_test, predictions)\n",
    "#plt.xlabel(“Values”)\n",
    "#plt.ylabel(“Predictions”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46616811 0.        ]\n",
      " [0.46172342 1.        ]\n",
      " [0.4633722  0.        ]\n",
      " ...\n",
      " [0.4654271  0.        ]\n",
      " [0.46775326 1.        ]\n",
      " [0.46524999 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print the predictions vs y_test value\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
